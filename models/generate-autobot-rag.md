# generate-autobot-rag.py Documentation

## Overview
This module provides generation functionality for the autobot-rag language model. It creates formatted prompts using the model's chat template, performs tokenization, configures sampling parameters, and generates text responses with detailed performance metrics.

## Function: `generate_autobot_response()`

### Purpose
Generates a complete, non-streaming text response from the autobot-rag model using a system message and user prompt. Handles the entire pipeline: prompt formatting, tokenization, generation configuration, model inference, and output cleaning.

### Input Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | `Any` | Loaded AutoModelForCausalLM instance (from load-autobot-rag) |
| `tokenizer` | `Any` | Loaded AutoTokenizer instance (from load-autobot-rag) |
| `system_message` | `str` | System role message defining model behavior/context (e.g., "You are a helpful assistant") |
| `user_prompt` | `str` | User's input text query to generate response for |
| `device` | `str` | Device for inference: `"cpu"` or `"cuda"` |
| `max_tokens` | `int` | Maximum number of tokens to generate (soft limit) |
| `temperature` | `float` | Sampling temperature (0.0 = deterministic, 1.0 = max randomness) |
| `max_context_length` | `int` | Maximum context window size supported by model (in tokens) |
| `max_tokens_hard_limit` | `int` | Hard upper limit on generated tokens (cannot exceed this value) |
| `model_label` | `str` | Optional identifier for logging purposes (default: `"autobot-rag"`) |

### Output

Returns a `Dict[str, Any]` containing:

| Key | Type | Description |
|-----|------|-------------|
| `text` | `str` | Cleaned generated text (special tokens removed, whitespace trimmed) |
| `raw_text` | `str` | Raw generated text with special tokens preserved |
| `template_token_count` | `int` | Number of tokens in the formatted prompt (before generation) |
| `formatted_prompt` | `str` | Full prompt after applying chat template (system + user messages) |
| `input_length` | `int` | Number of tokens in the tokenized input |
| `generated_tokens` | `int` | Actual number of tokens generated by the model |

### How It Works

#### Phase 1: Message Structure
```
messages = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
]
```
Creates a conversation structure with system context and user query.

#### Phase 2: Chat Template Application
- Applies the tokenizer's `chat_template` to format messages for the specific model
- `tokenize=False`: Returns formatted text string, not tokens yet
- `add_generation_prompt=True`: Adds appropriate prompt markers for generation mode
- Logs formatted prompt length in characters

#### Phase 3: Token Counting
- Encodes formatted prompt to count actual tokens
- Uses `add_special_tokens=False` for accurate counting
- Logs template token count for debugging

#### Phase 4: Input Tokenization
- Tokenizes the formatted prompt with truncation
- `truncation=True`: Truncates if exceeds `max_context_length - max_tokens`
- Returns PyTorch tensors on specified device
- Logs tokenized input length in tokens

#### Phase 5: Generation Configuration
Dynamic configuration based on temperature:

**If temperature â‰¤ 0.1 (Deterministic mode):**
- `do_sample=False`: Greedy decoding (select highest probability token)
- Sampling parameters (top_p, top_k) removed

**If temperature > 0.1 (Sampling mode):**
- `do_sample=True`: Probabilistic sampling
- `temperature`: Adjusted to range [0.1, 1.0]
- `top_p=0.1`: Nucleus sampling with 10% probability mass
- `top_k=50`: Consider top 50 tokens
- `repetition_penalty=1.05`: Penalize token repetition
- `no_repeat_ngram_size=3`: Prevent repeating 3-grams

**Always includes:**
- `max_new_tokens`: `min(max_tokens, max_tokens_hard_limit)`
- `pad_token_id`: From tokenizer
- `eos_token_id`: From tokenizer (for proper stopping)
- `use_cache=True`: KV cache for faster inference

#### Phase 6: Model Generation
- Runs inference with `torch.no_grad()` for memory efficiency
- Measures elapsed time
- Extracts only generated tokens (skips input tokens)
- Logs generation speed in tokens per second (TPS)

#### Phase 7: Output Cleaning
- Decodes generated token IDs to text
- Removes special tokens: `<|im_end|>`, `<|im_start|>`, `<|endoftext|>`, `<|startoftext|>`
- Strips leading/trailing whitespace
- Preserves raw output for debugging

### Logging Output

The function provides detailed console logging with `[generate-autobot-rag]` prefix:

- Function call confirmation with model label and device
- Generation configuration (max_tokens, temperature)
- Chat template application status
- Formatted prompt length
- Template and input token counts
- Generation mode (deterministic or sampling)
- Generation progress and completion with timing stats
- Final response payload confirmation

### Constants

```python
SPECIAL_TOKENS = ["<|im_end|>", "<|im_start|>", "<|endoftext|>", "<|startoftext|>"]
```
Tokens automatically removed from the cleaned output text.

### Dependencies

- `torch`: PyTorch for tensor operations and model inference
- `transformers`: Model and tokenizer utilities
- `time`: For performance measurement

### Example Usage

```python
from load_autobot_rag import load_autobot_rag
from generate_autobot_rag import generate_autobot_response

# Load model
tokenizer, model = load_autobot_rag("./autobot-rag/", device="cuda")

# Generate response
response = generate_autobot_response(
    model=model,
    tokenizer=tokenizer,
    system_message="You are a helpful programming assistant.",
    user_prompt="What is Python?",
    device="cuda",
    max_tokens=256,
    temperature=0.7,
    max_context_length=4096,
    max_tokens_hard_limit=512,
    model_label="autobot-rag"
)

print("Generated text:", response["text"])
print("Tokens generated:", response["generated_tokens"])
```

### Performance Parameters

| Parameter | Deterministic | Sampling | Purpose |
|-----------|---------------|----------|---------|
| `do_sample` | False | True | Enable probabilistic sampling |
| `temperature` | N/A | 0.1-1.0 | Control randomness |
| `top_p` | N/A | 0.1 | Nucleus sampling threshold |
| `top_k` | N/A | 50 | Limit candidate tokens |
| `repetition_penalty` | N/A | 1.05 | Penalize repeats |
| `no_repeat_ngram_size` | N/A | 3 | Block repeated patterns |

### Key Behaviors

1. **Context Window Management**: Input is truncated to `max_context_length - max_tokens` to ensure space for generation

2. **Token Limit Enforcement**: Generated tokens are capped by `min(max_tokens, max_tokens_hard_limit)`

3. **Device Optimization**: Uses `torch.no_grad()` to disable gradient computation and reduce memory usage

4. **Deterministic vs. Sampling**:
   - `temperature=0.0` to `0.1`: Greedy decoding (reproducible)
   - `temperature > 0.1`: Probabilistic sampling (varied outputs)

5. **Special Token Handling**: 
   - Raw output includes all tokens
   - Cleaned output removes special markers
   - Both are returned for debugging flexibility

### Return Example

```python
{
    "text": "Python is a high-level programming language known for its simplicity.",
    "raw_text": "<|im_end|>Python is a high-level programming language<|endoftext|>",
    "template_token_count": 42,
    "formatted_prompt": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nWhat is Python?<|im_end|>\n<|im_start|>assistant\n",
    "input_length": 42,
    "generated_tokens": 15
}
```

### Important Notes

- Function requires both model and tokenizer to be properly loaded via `load_autobot_rag()`
- Using `temperature=0` or very low values ensures reproducible outputs
- Higher temperatures (>0.7) increase creativity but may reduce coherence
- The `max_tokens_hard_limit` provides a safety override on user-specified `max_tokens`
- All outputs are on the specified device; move to CPU if needed for serialization
