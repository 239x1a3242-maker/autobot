# Load Autobot Thinking - Documentation

## Overview

The `load_autobot_thinking.py` module is responsible for **loading and initializing the Autobot Thinking model and tokenizer** from local storage. It handles device detection (CUDA/CPU), proper data type configuration, and provides utilities to access the loaded models.

---

## Purpose

- **Single responsibility:** Model and tokenizer loading
- **Device awareness:** Automatically detects CUDA and configures appropriate precision
- **Error handling:** Comprehensive logging and fallback mechanisms
- **Reusability:** Singleton-like pattern to prevent duplicate loading

---

## File Location

### Directory Structure
```
models/
‚îú‚îÄ‚îÄ autobot-thinking/              # Model weights directory
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors          # Model weights
‚îÇ   ‚îú‚îÄ‚îÄ config.json                # Model config
‚îÇ   ‚îú‚îÄ‚îÄ generation_config.json     # Generation settings
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json             # Tokenizer data
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json      # Tokenizer config
‚îÇ   ‚îú‚îÄ‚îÄ special_tokens_map.json    # Special tokens
‚îÇ   ‚îî‚îÄ‚îÄ chat_template.jinja        # Chat template
‚îî‚îÄ‚îÄ load_autobot_thinking.py       # ‚Üê This file
```

### This Module Only
**This documentation covers ONLY `load_autobot_thinking.py`**
- Standalone module for model loading
- Self-contained, no external integrations
- Independent function that loads Autobot Thinking model and tokenizer

---

## API Reference

### 1. `load_autobot_thinking_model() ‚Üí Tuple[Any, Any, bool]`

**Purpose:** Main function to load the model and tokenizer

**Returns:**
- `tokenizer` (AutoTokenizer): Loaded tokenizer instance
- `model` (AutoModelForCausalLM): Loaded model instance  
- `success` (bool): True if loading succeeded, False if failed

**Device Configuration:**
- **CUDA available:** Uses `bfloat16` (better precision, faster)
- **CPU only:** Uses `float32` (fallback for memory)

**Usage Example:**

```python
from models.load_autobot_thinking import load_autobot_thinking_model

# Load the model and tokenizer
tokenizer, model, success = load_autobot_thinking_model()

if success:
    print("‚úÖ Model loaded successfully!")
    print(f"Device: {model.device}")
    print(f"Data type: {model.dtype}")
else:
    print("‚ùå Failed to load model")
```

---

### 2. `get_model() ‚Üí Optional[Any]`

**Purpose:** Get the currently loaded model instance

**Returns:**
- Model if loaded, `None` if not loaded

**Usage Example:**

```python
from models.load_autobot_thinking import get_model

model = get_model()
if model:
    print(f"Model is available: {type(model)}")
else:
    print("Model not loaded yet")
```

---

### 3. `get_tokenizer() ‚Üí Optional[Any]`

**Purpose:** Get the currently loaded tokenizer instance

**Returns:**
- Tokenizer if loaded, `None` if not loaded

**Usage Example:**

```python
from models.load_autobot_thinking import get_tokenizer

tokenizer = get_tokenizer()
if tokenizer:
    print(f"Tokenizer vocab size: {len(tokenizer)}")
else:
    print("Tokenizer not loaded yet")
```

---

### 4. `is_model_loaded() ‚Üí bool`

**Purpose:** Check if model is currently loaded in memory

**Returns:**
- `True` if model is loaded
- `False` if model is not loaded

**Usage Example:**

```python
from models.load_autobot_thinking import is_model_loaded

if is_model_loaded():
    print("‚úÖ Model is ready to use")
else:
    print("‚ö†Ô∏è Model needs to be loaded first")
```

---

### 5. `get_device_info() ‚Üí Dict[str, Any]`

**Purpose:** Get detailed device and model configuration information

**Returns:**
```python
{
    "device": "cuda",                      # or "cpu"
    "device_index": 0,                     # GPU index if CUDA
    "cuda_available": True,                # bool
    "device_name": "NVIDIA H100",          # GPU name
    "cuda_version": "12.1",                # CUDA version
    "torch_version": "2.1.2",              # PyTorch version
    "using_dtype": "bfloat16",             # or "float32"
    "model_dtype": "bfloat16",             # Actual model dtype
    "model_device": "cuda:0"               # Full device info
}
```

**Usage Example:**

```python
from models.load_autobot_thinking import get_device_info

info = get_device_info()
print(f"Running on: {info['device_name']}")
print(f"Precision: {info['using_dtype']}")
print(f"CUDA Version: {info['cuda_version']}")
```

---

## Direct Usage Example

Here's how to use this module directly in the models directory:

### Simple Direct Import
```python
from load_autobot_thinking import load_autobot_thinking_model

# Load the model
tokenizer, model, success = load_autobot_thinking_model()

if success:
    print("‚úÖ Model loaded!")
else:
    print("‚ùå Load failed")
```

### With Dynamic Import
```python
import sys
import importlib.util
from pathlib import Path

# Load this module dynamically
module_path = Path(__file__).parent / 'load_autobot_thinking.py'
spec = importlib.util.spec_from_file_location("load_autobot_thinking", str(module_path))
load_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(load_module)

# Use the module
tokenizer, model, success = load_module.load_autobot_thinking_model()
```

### Standalone Script Example
```python
#!/usr/bin/env python3
# models/test_loader.py

from load_autobot_thinking import (
    load_autobot_thinking_model,
    get_device_info,
    is_model_loaded
)

if __name__ == "__main__":
    print("üîÑ Loading models...")
    tokenizer, model, success = load_autobot_thinking_model()
    
    if not success:
        print("‚ùå Failed to load model")
        exit(1)
    
    print("‚úÖ Models loaded successfully!")
    
    info = get_device_info()
    print(f"   Device: {info['device']}")
    print(f"   GPU: {info.get('device_name', 'N/A')}")
    print(f"   Precision: {info['using_dtype']}")
    print(f"   Loaded: {is_model_loaded()}")
```

---

## Module Output Examples

### Output 1: Successful Load
```
üîÑ Loading models...
‚úÖ Models loaded successfully!
   Device: cuda
   GPU: NVIDIA H100
   Precision: bfloat16
   Loaded: True
```

### Output 2: Tokenizer Info
```python
tokenizer = load_autobot_thinking_model()[0]
print(f"Vocab size: {len(tokenizer)}")
print(f"Chat template available: {tokenizer.chat_template is not None}")
```

### Output 3: Device Info Dict
```python
{
    "device": "cuda",
    "device_index": 0,
    "cuda_available": true,
    "device_name": "NVIDIA H100",
    "cuda_version": "12.1",
    "torch_version": "2.1.2",
    "using_dtype": "bfloat16",
    "model_dtype": "bfloat16",
    "model_device": "cuda:0"
}
```

---

## Internal Working

### Loading Process

1. **Device Detection**
   ```python
   device = "cuda" if torch.cuda.is_available() else "cpu"
   ```
   - Checks if NVIDIA GPU is available
   - Falls back to CPU if CUDA not available

2. **Precision Setting**
   ```python
   if device == "cuda":
       dtype = torch.bfloat16  # Better precision
   else:
       dtype = torch.float32   # CPU fallback
   ```

3. **Tokenizer Loading**
   ```python
   tokenizer = AutoTokenizer.from_pretrained(
       model_path,
       trust_remote_code=True
   )
   ```
   - Loads tokenizer configuration
   - Initializes special tokens

4. **Model Loading**
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_path,
       torch_dtype=dtype,
       device_map="auto",
       trust_remote_code=True
   )
   ```
   - Loads model weights from disk
   - Configures appropriate precision
   - Optimizes for device

5. **Logging**
   - 6 logging checkpoints throughout the process
   - Tracks device info, tokenizer, model config, and final status

### Error Handling

**Double Try-Catch Mechanism:**

```python
try:
    # First attempt - try standard loading
    model = load_with_default()
except Exception as e:
    # Fallback - try alternative loading
    model = load_with_fallback()
```

- First attempt uses optimal settings
- Fallback uses more conservative settings
- Both attempts logged for debugging

### Global Variables

The module maintains two global variables:

```python
_autobot_tokenizer = None    # Cached tokenizer
_autobot_model = None        # Cached model
```

- Prevents duplicate loading
- Enables multiple function calls
- Memory efficient

---

## Configuration

### Model Path
```python
AUTOBOT_MODEL_NAME = "./models/autobot-thinking"
```

Current directory structure expected:
- `./models/autobot-thinking/` - Model weights
- `./models/load_autobot_thinking.py` - This loader

### Supported Models
- Autobot Thinking (Official)
- Any compatible Autobot Thinking variant

### Data Types

| Device | Data Type | Advantage | Use Case |
|--------|-----------|-----------|----------|
| CUDA   | bfloat16  | Speed, precision | GPU with FP32 support |
| CPU    | float32   | Compatibility | CPU-only systems |

---

## Common Issues & Solutions

### Issue 1: CUDA Out of Memory

**Error:** `torch.cuda.OutOfMemoryError`

**Solution:**
```python
# Use CPU instead
import torch
torch.cuda.is_available = lambda: False

tokenizer, model, success = load_autobot_thinking_model()
```

### Issue 2: Model Not Found

**Error:** `FileNotFoundError: ./models/autobot-thinking/`

**Solution:**
Ensure model files are in correct location:
```
your_project/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ autobot-thinking/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (other files)
‚îÇ   ‚îî‚îÄ‚îÄ load_autobot_thinking.py
‚îî‚îÄ‚îÄ app.py
```

### Issue 3: Slow Loading

This is normal for first load (can take 30-60 seconds):
- First load: Downloads and caches model
- Subsequent loads: Much faster (uses cache)

---

## Performance Metrics

### Load Time
- **CUDA (first load):** 30-60 seconds
- **CUDA (cached):** 5-10 seconds
- **CPU:** 45-90 seconds

### Memory Usage
- **Model:** ~2.5 GB GPU VRAM
- **Tokenizer:** ~50 MB
- **Total:** ~2.6 GB

### Inference Speed
- **CUDA (bfloat16):** ~20-30 tokens/sec
- **CPU (float32):** ~2-5 tokens/sec

---

## Best Practices

1. **Load Once at Startup**
   ```python
   # ‚ùå Bad: Load in every request
   def handle_request():
       tokenizer, model, _ = load_autobot_thinking_model()
   
   # ‚úÖ Good: Load once at app startup
   tokenizer, model, success = load_autobot_thinking_model()
   
   def handle_request():
       # Use global tokenizer and model
   ```

2. **Check Loading Status**
   ```python
   # ‚úÖ Always verify loading succeeded
   tokenizer, model, success = load_autobot_thinking_model()
   if not success:
       raise RuntimeError("Model loading failed")
   ```

3. **Use Device Abstraction**
   ```python
   # ‚úÖ Don't hardcode device
   device_info = get_device_info()
   device = device_info['device']
   ```

4. **Handle Cache Misses**
   ```python
   # ‚úÖ Check if loaded before use
   if not is_model_loaded():
       load_autobot_thinking_model()
   ```

---

## Summary

| Aspect | Details |
|--------|---------|
| **Purpose** | Load Autobot Thinking model and tokenizer |
| **Main Function** | `load_autobot_thinking_model()` |
| **Return Type** | `(tokenizer, model, success: bool)` |
| **Device Support** | CUDA (GPU) + CPU (fallback) |
| **Load Time** | 5-60 seconds depending on device |
| **Memory Usage** | ~2.6 GB |
| **Precision** | bfloat16 (GPU) or float32 (CPU) |
| **Error Handling** | Comprehensive with logging |
| **Caching** | Global variables to prevent reload |

